{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA1: Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name:\n",
    "### Roll Number:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Outcomes\n",
    "\n",
    "After this PA you can:\n",
    "\n",
    "- Choose Naive Bayes variant based on feature type  \n",
    "- Implement Naive Bayes in log-space  \n",
    "- Design a Bag-of-Words (BoW) vectorizer  \n",
    "- Evaluate models with macro metrics  \n",
    "- Compare implementations with scikit-learn  \n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will be implementing two types of Naive Bayes model based on the dataset features and task requirements.\n",
    "\n",
    "For reference and additional details on these topics, please go through [Appendix B](https://web.stanford.edu/~jurafsky/slp3/) of the SLP3 book.\n",
    "\n",
    "In this assignment, you are provided with two datasets. One is suitable for **Multinomial Naive Bayes**, while the other is appropriate for **Bernoulli Naive Bayes**. Your task is to:\n",
    "1. Analyze both datasets and determine which Naive Bayes model to apply based on the datasetâ€™s characteristics.\n",
    "2. Implement both **Multinomial** and **Bernoulli Naive Bayes** from scratch, adhering to the guidelines below regarding allowed libraries.\n",
    "3. Finally, apply the corresponding models using the `sklearn` library and compare the results with your own implementation.\n",
    "\n",
    "### Import Policy Clarification\n",
    "\n",
    "To keep the assignment consistent and fair, please follow these rules for imports:\n",
    "\n",
    "- **From-scratch sections**:  \n",
    "  - Allowed: `numpy`, `pandas`, and Python standard library (e.g., `import re`)  \n",
    "  - Do **not** use `sklearn` or any other ML libraries.\n",
    "\n",
    "- **Evaluation (anywhere in the notebook)**:  \n",
    "  - Allowed: `sklearn.metrics` (e.g., `accuracy_score`, `precision_score`, `recall_score`, etc.)\n",
    "\n",
    "- **Baselines section only**:  \n",
    "  - Allowed: `sklearn.naive_bayes` (specifically `MultinomialNB` and `BernoulliNB`)  \n",
    "\n",
    "**No additional imports** are permitted beyond the scope listed above.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Feel free to insert additional code cells.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All necessary libraries for this assignment have already been added. You are not allowed to add any additional imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line if scikit-learn not installed already\n",
    "# !pip install  scikit-learn --quiet\n",
    "\n",
    "# !pip install numpy regex pandas --quiet\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Datasets\n",
    "\n",
    "In this assignment, you are provided with two datasets:\n",
    "\n",
    "- **Mushroom**: The dataset contains descriptions of over 8,000 mushrooms. Each mushroom is labeled as either edible (e) or poisonous (p), and the dateset includes 22 categorical features such as cap shape, color, odor, gill size, and habitat for each datapoint. Read more about the dataset [here](https://www.kaggle.com/datasets/uciml/mushroom-classification/data).   \n",
    "- **AG-News**: The AG News dataset is a widely used benchmark for text classification, containing over 120,000 news articles grouped into four categories: World, Sports, Business, and Science/Technology. Each article consists of a short news title and description. Furthermore a test-train split has already been done for you. Read more about the dataset [here](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset). In this assignment you will be using 20k articles for training and 5k articles for testing.\n",
    "\n",
    "### Task [3 Marks]:\n",
    "- Explore both datasets and identify their key features. This will help you determine which dataset is best suited for **Multinomial Naive Bayes** and which is better suited for **Bernoulli Naive Bayes**. \n",
    "- Load both Datasets (using pandas) and Print the first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mushroom Dataset\n",
    "# Code Here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AG-news Dataset\n",
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before proceeding with further tasks, ensure you have determined which type of Naive Bayes is most suitable for each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing [10 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing the Mushroom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will first apply one-hot encoding to the categorical columns of the Mushroom dataset.\n",
    " After encoding, you will split the data into training and test sets using [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from `sklearn`. Make sure to set the parameter `test_size=0.3`, which means that 30% of the data will be used for testing and 70% for training. \n",
    "\n",
    "Tip: It is helpful to print the **shape of the dataset before splitting** and the **shapes of the training and test sets after splitting**. This will let you confirm that the split has been done correctly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing the AG-News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, you need to pre-process your data to ensure it's in a clean format for further analysis. The following steps should be performed:\n",
    "\n",
    "- Remove any URL.\n",
    "- Remove punctuation and non-alphanumeric characters.\n",
    "- Convert all text to lowercase.\n",
    "- Remove any extra whitespace.\n",
    "- Eliminate common stopwords.\n",
    "\n",
    "In the cell below, implement a function that carries out these tasks. You can use the `english_stopwords.txt` for removing stopwords.\n",
    "\n",
    "\n",
    "Note: Stop words are very common words (such as *the, is, and, in, of*) that occur frequently in text but usually do not add much meaning for tasks like classification. Removing stop words helps reduce noise in the data and makes the model focus on the more informative words.\n",
    "\n",
    "Once the function is complete, apply it to the `Description` column of your dataset to obtain the preprocessed text.\n",
    "\n",
    "Tip: Use different variable names from the variables you used in 2.1 so they don't get overwritten \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 5 data samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Naive Bayes from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Bernoulli Naive Bayes [25 Marks]\n",
    "\n",
    "### From Scratch \n",
    "\n",
    "Recall that the Bernoulli Naive Bayes model is based on **Bayes' Theorem**:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "What we really want is to find the class \\(c\\) that maximizes $P(c \\mid x)$, so we can use the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(x \\mid c)P(c)\n",
    "$$\n",
    "\n",
    "In the case of **Bernoulli Naive Bayes**, we assume that each word \\(x_i\\) in a sentence follows a **Bernoulli distribution**, meaning that the word either appears (1) or does not appear (0) in the document. We can simplify the formula using this assumption:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i = 1 \\mid c)^{x_i} P(x_i = 0 \\mid c)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x_i = 1$ if the $i^{\\text{th}}$ word is present in the document.\n",
    "- $x_i = 0$ if the $i^{\\text{th}}$ word is not present in the document.\n",
    "\n",
    "\n",
    "We can estimate $P(c)$ by counting the number of times each class appears in our training data, and dividing by the total number of training examples. We can estimate $P(x_i = 1 \\mid c)$ by counting the number of documents in class $c$ that contain the word $x_i$, and dividing by the total number of documents in class $c$.\n",
    "\n",
    "### **Important: Laplace Smoothing**\n",
    "\n",
    "When calculating $P(x_i = 1 \\mid c)$ and $P(x_i = 0 \\mid c)$, we apply **Laplace smoothing** to avoid zero probabilities. This is essential because, without it, any word that has not appeared in a document of class $c$ will have a probability of zero, which would make the overall product zero, leading to incorrect classification.\n",
    "\n",
    "**Reason**: Laplace smoothing ensures that we don't encounter zero probabilities by adding a small constant (typically 1) to both the numerator and the denominator. This is particularly useful when a word has never appeared in the training data for a specific class.\n",
    "\n",
    "The smoothed probability formula is:\n",
    "\n",
    "$$\n",
    "P(x_i = 1 \\mid c) = \\frac{\\text{count of documents in class } c \\text{ where } x_i = 1 + 1}{\\text{total documents in class } c + 2}\n",
    "$$\n",
    "\n",
    "This ensures no word has a zero probability, even if it was unseen in the training data.\n",
    "\n",
    "### Avoiding Underflow with Logarithms:\n",
    "\n",
    "To avoid underflow errors due to multiplying small probabilities, we apply logarithms, which convert the product into a sum:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ \\log P(c) + \\sum_{i=1}^{n} \\left[ x_i \\log P(x_i = 1 \\mid c) + (1 - x_i) \\log P(x_i = 0 \\mid c) \\right]\n",
    "$$\n",
    "\n",
    "You will now implement this algorithm.\n",
    "\n",
    "<span style=\"color: red;\"> For this part, the only external library you will need is `numpy`. You are not allowed to use anything else.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a **class definition** for `BernoulliNaiveBayes` below. Your task is to complete the methods so that the class can learn from training data and make predictions on new data.\n",
    "\n",
    "### 1. `__init__`  \n",
    "This method sets up the model.  \n",
    "It creates empty containers that will later hold:  \n",
    "- the probabilities of each class (class priors), and  \n",
    "- the probabilities of features given each class.  \n",
    "\n",
    "At this stage, no calculations are done â€” itâ€™s just preparation.\n",
    "\n",
    "\n",
    "### 2. `fit(X_train, y_train)`  \n",
    "This method is where the model learns from the training data.  \n",
    "\n",
    "It figures out:  \n",
    "- how likely each class is overall, and  \n",
    "- how likely each feature is to appear (or not) when the data belongs to a certain class.  \n",
    "\n",
    "These probabilities are estimated from the training data, with a small adjustment (Laplace smoothing) so that unseen features donâ€™t break the model.  \n",
    "\n",
    "After this step, the model has all the information it needs for making predictions.\n",
    "\n",
    "### 3. `predict(X_test)`  \n",
    "This method uses the probabilities learned during training to classify new examples.  \n",
    "\n",
    "For each new input, the model:  \n",
    "- calculates how likely it is to belong to each possible class, and  \n",
    "- chooses the class with the highest likelihood.  \n",
    "\n",
    "This is how the model turns what it learned into actual predictions.\n",
    "###  In Short  \n",
    "- `__init__`: prepare storage for probabilities.  \n",
    "- `fit`: learn probabilities from training data.  \n",
    "- `predict`: apply those probabilities to classify new data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes:\n",
    "    def __init__(self):\n",
    "       pass\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        pass \n",
    "\n",
    "    def predict(self, X_test):\n",
    "       pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¹ What you need to do:\n",
    "1. **Train the model**  \n",
    "   - Create an object of your `BernoulliNaiveBayes` class.  \n",
    "   - Call the `.fit()` method on your training data (`X_train`, `y_train`).  \n",
    "\n",
    "2. **Generate predictions**  \n",
    "   - Use the `.predict()` method on your test data (`X_test`).  \n",
    "   - Store the predicted labels in `y_pred`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the model**  \n",
    "   -  the code to calculate evaluation metrics and display the confusion matrix has been provided to you.  \n",
    "   - All you need to do is **replace the dummy variables** in the code with your actual ones:  \n",
    "     - `y_test` â†’ the true labels from your test set.  \n",
    "     - `y_pred` â†’ the labels predicted by your model.  \n",
    "   - The code will then automatically compute **Accuracy**  **Precision**  **Recall**  **F1 Score**  and display the **Confusion Matrix**. \n",
    "###  Important:\n",
    "For now, just remember:  \n",
    "- **The higher these scores are, the better your model is performing.**  \n",
    "- Each metric should be at least **86% or higher** on the validation data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace y_test,y_pred with ur test variables\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix_display = ConfusionMatrixDisplay(matrix)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "matrix_display.plot()\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multinomial Naive Bayes (Manual Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing sentences with Bag of Words [7 Marks]\n",
    "\n",
    "Now that we have loaded in our data, we will need to vectorize our sentences - this is necessary to be able to numericalize our inputs before feeding them into our model. \n",
    "\n",
    "We will be using a Bag of Words approach to vectorize our sentences. This is a simple approach that counts the number of times each word appears in a sentence. \n",
    "\n",
    "The element at index $\\text{i}$ of the vector will be the number of times the $\\text{i}^{\\text{th}}$ word in our vocabulary appears in the sentence. So, for example, if our vocabulary is `[\"the\", \"cat\", \"sat\", \"on\", \"mat\"]`, and our sentence is `\"the cat sat on the mat\"`, then our vector will be `[2, 1, 1, 1, 1]`.\n",
    "\n",
    "You will now create a `BagOfWords` class to vectorize our sentences. This will involve creating\n",
    "\n",
    "1. A vocabulary from our corpus\n",
    "\n",
    "2. A mapping from words to indices in our vocabulary\n",
    "\n",
    "3. A function to vectorize a sentence in the fashion described above\n",
    "\n",
    "It may help you to define something along the lines of a `fit` and a `vectorize` method.\n",
    "\n",
    "- **`fit(sentences)`**: This method should look at all the sentences in your dataset (the *corpus*) and build the vocabulary. It also creates a mapping from each word to its index in the vocabulary.  \n",
    "\n",
    "- **`vectorize(sentence)`**: This method takes a single sentence and converts it into a Bag of Words vector, using the vocabulary created in `fit`. Each position in the vector corresponds to a word in the vocabulary, and the value is the count of how many times that word appears in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        pass\n",
    "\n",
    "    def vectorize(self, sentence):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sanity check, you can manually set the vocabulary of your `BagOfWords` object to the vocabulary of the example above, and check that the vectorization of the sentence is correct.\n",
    "\n",
    "Once you have implemented the `BagOfWords` class, fit it to the training data, and vectorize the training,and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### From Scratch [30 Marks]\n",
    "\n",
    "Now that we have vectorized our sentences, we can implement our Naive Bayes model. Recall that the Naive Bayes model is based off of the Bayes Theorem:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "What we really want is to find the class $c$ that maximizes $P(c \\mid x)$, so we can use the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(x \\mid c)P(c)\n",
    "$$\n",
    "\n",
    "We can then use the Naive Bayes assumption to simplify this:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i \\mid c)\n",
    "$$\n",
    "\n",
    "Where $x_i$ is the $i^{\\text{th}}$ word in our sentence.\n",
    "\n",
    "All of these probabilities can be estimated from our training data. We can estimate $P(c)$ by counting the number of times each class appears in our training data, and dividing by the total number of training examples. We can estimate $P(x_i \\mid c)$ by counting the number of times the $i^{\\text{th}}$ word in our vocabulary appears in sentences of class $c$, and dividing by the total number of words in sentences of class $c$.\n",
    "\n",
    "It would help to apply logarithms to the above equation so that we translate the product into a sum, and avoid underflow errors. This will give us the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ \\log P(c) + \\sum_{i=1}^{n} \\log P(x_i \\mid c)\n",
    "$$\n",
    "\n",
    "You will now implement this algorithm. It would help to go through [Appendix B](https://web.stanford.edu/~jurafsky/slp3) to get a better understanding of the model - **it is recommended base your implementation off the pseudocode that has been provided**. You can either make a `NaiveBayes` class, or just implement the algorithm across two functions.\n",
    "\n",
    "<span style=\"color: red;\"> For this part, the only external library you will need is `numpy`. You are not allowed to use anything else.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your implementation to train a Naive Bayes model on the training data, and generate predictions for the Validation Set.\n",
    "\n",
    "Report the Accuracy, Precision, Recall, and F1 score of your model on the validation data. Also display the Confusion Matrix. You are allowed to use `sklearn.metrics` for this.\n",
    "\n",
    "#### All of your metrics should have a score of 86% or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace y_test,y_pred with ur test variables\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix_display = ConfusionMatrixDisplay(matrix)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "matrix_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Naive Bayes using sklearn [15 Marks]\n",
    "\n",
    "In this section, you will compare your manual implementation with **scikit-learn**'s (`sklearn`) implementation of the Naive Bayes models we covered above.\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) (also called `sklearn`) is one of the most widely used machine learning libraries in Python.  \n",
    "It provides:\n",
    "- **Pre-built, efficient model classes** that you can directly import and use without writing the low-level code yourself.\n",
    "- Tools for **training**, **testing**, and **evaluating** models.  \n",
    "\n",
    "In sklearn, most models follow the same basic pattern:\n",
    "1. **Import the model** you want to use.  \n",
    "2. **Create an object** of that model class.  \n",
    "3. **Train the model** using the `.fit()` method (on your training data).  \n",
    "4. **Make predictions** using the `.predict()` method (on your test data).  \n",
    "5. **Evaluate performance** using metrics such as accuracy.  \n",
    "\n",
    "For Naive Bayes, sklearn provides different variants depending on the type of data:\n",
    "- [`BernoulliNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) â€“ used when the features are binary\n",
    "- [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) â€“ used when the features are discrete or represent counts.\n",
    "\n",
    "**Your Task**  \n",
    "1. Implement both `BernoulliNB` and `MultinomialNB` using sklearn on the same datasets you used for your manual implementation.  \n",
    "2. Evaluate your models using the following metrics(code given-just replace the dummy variables):  \n",
    "   - **Accuracy**  \n",
    "   - **Precision**  \n",
    "   - **Recall**  \n",
    "   - **F1-score**  \n",
    "   - **Confusion Matrix**  \n",
    "3. Compare the performance of these sklearn models with your manual implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here for MultiNomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix_display = ConfusionMatrixDisplay(matrix)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "matrix_display.plot()\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here for BernoulliNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix_display = ConfusionMatrixDisplay(matrix)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "matrix_display.plot()\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion [5 Marks]\n",
    "\n",
    "1. Explain the key factors you considered when determining which dataset is more suitable for **Multinomial Naive Bayes** and which is better suited for **Bernoulli Naive Bayes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation using Naive Bayes [5 marks]\n",
    "\n",
    " ### Shannon-Style Text Generation with Naive Bayes\n",
    "\n",
    "So far, we have used **Multinomial Naive Bayes** as a *classifier*:  \n",
    "- Given a text â†’ predict the most likely **class**.  \n",
    "\n",
    "But we can also flip the perspective:  \n",
    "- Given a **class** â†’ generate a sequence of words that \"look like\" that class.  \n",
    "\n",
    "This idea goes back to **Claude Shannon (1948)**, who showed how to generate language-like sequences by **sampling symbols according to their probabilities**.  \n",
    "\n",
    "\n",
    "### Shannonâ€™s Algorithm (Simplified)\n",
    "\n",
    " \n",
    "1. **Collect statistics**: estimate how often each symbol (word or character) appears.  \n",
    "2. **Build probabilities**: compute the distribution of symbols.  \n",
    "3. **Sample from the distribution**:  \n",
    "   - At each step, randomly choose the next symbol according to its probability.  \n",
    "   - More frequent symbols are more likely to appear, but rare ones can still show up.  \n",
    "4. **Repeat** this process to generate longer sequences.  \n",
    "\n",
    "The result may look like gibberish, but it reflects the *statistical flavor* of the original text.  \n",
    "\n",
    "###  Connecting Shannon to Naive Bayes\n",
    "\n",
    "- In our case, the \"statistics\" come from the **trained `MultinomialNB` model in scikit-learn**.  \n",
    "- For each class, the model stores:  \n",
    "  - `model.feature_log_prob_` â†’ the probability of each word given that class.  \n",
    "- By picking a class and sampling words from its probability distribution, we implement **Shannonâ€™s idea of random generation**.  \n",
    "- For example:  \n",
    "  - Class = `\"Sports\"`  \n",
    "  - High-probability words might be: `[\"team\", \"game\", \"season\", \"player\"]`  \n",
    "  - Generated text could be:  \n",
    "    ```\n",
    "    player game team season coach win team league\n",
    "    ```  \n",
    "\n",
    "\n",
    "###  Why is this important?\n",
    "\n",
    "- It shows that even a simple classifier like Naive Bayes can be used *generatively*.  \n",
    "- This connects the historical Shannon method to modern AI: both rely on sampling from probability distributions.  \n",
    "- Advanced models (like GPT) use the same principle, but with much richer probability structures and long-range context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shannon_generate(model, vocab, label_idx, n_words=10):\n",
    "    \"\"\"\n",
    "    Generate random words from a trained MultinomialNB for a given class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : MultinomialNB\n",
    "        A trained Naive Bayes model from scikit-learn.\n",
    "    vocab : list\n",
    "        The vocabulary list (mapping feature indices back to words).\n",
    "    label_idx : int\n",
    "        Index of the class label we want to generate text for.\n",
    "    n_words : int\n",
    "        Number of words to sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sampled_words : list\n",
    "        List of generated words.\n",
    "    probs : np.array\n",
    "        Probability distribution of words for the chosen class.\n",
    "    \"\"\"\n",
    "\n",
    "    # === Step 1: Get log-probabilities of words for the chosen class ===\n",
    "    log_probs = model.feature_log_prob_[label_idx]\n",
    "\n",
    "    # === Step 2: Convert log-probabilities back to normal probabilities ===\n",
    "    probs = np.exp(log_probs)\n",
    "\n",
    "    # === Step 3: Normalize (just in case, ensures they sum to 1) ===\n",
    "    probs /= probs.sum()\n",
    "\n",
    "    # === Step 4: Randomly sample words according to their probabilities ===\n",
    "    # np.random.choice picks indices with probability proportional to probs\n",
    "    sampled_idx = np.random.choice(len(vocab), size=n_words, p=probs)\n",
    "\n",
    "    # === Step 5: Map sampled indices back to actual words from the vocab ===\n",
    "    sampled_words = [vocab[i] for i in sampled_idx]\n",
    "\n",
    "    return sampled_words, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generating Words from Each Class \n",
    "\n",
    "Weâ€™ve already given you the function `shannon_generate` (above).  \n",
    "Your job is to use it to **generate 10 random words per class** from the trained Naive Bayes model.  \n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. Loop through all the classes of your model.  \n",
    "2. For each class:\n",
    "Call `shannon_generate(model, vocab, label_idx, n_words=10)`.  \n",
    "Print the class name and the 10 generated words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'mnb' with your scikit-learn trained MultiNomial Naive Bayes\n",
    "label_names = mnb.classes_\n",
    "\n",
    "# Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Bonus (Optional):\n",
    "## Bi-gram Naive Bayes [15 Marks]\n",
    "\n",
    "So far, weâ€™ve worked with **unigrams** (single words).  \n",
    "Now weâ€™ll extend this idea to **bigrams** â€” pairs of consecutive words.\n",
    "\n",
    "###  What is a Bigram?\n",
    "- A **unigram** feature looks at individual words:  \n",
    "  `[\"i\", \"love\", \"pizza\"]`  \n",
    "- A **bigram** feature looks at pairs of words:  \n",
    "  `[(\"i\",\"love\"), (\"love\",\"pizza\")]`  \n",
    "\n",
    "Bigrams capture **local context** and word order. For example:\n",
    "- `\"not good\"` has a very different meaning than `\"good\"`.  \n",
    "- A unigram model might miss this, but a bigram model can capture it.\n",
    "\n",
    "###  How does Bigram Naive Bayes work?\n",
    "- We treat each **bigram** as a feature (instead of each single word).  \n",
    "- For each class:\n",
    "  - We count how often each bigram appears.  \n",
    "  - We compute probabilities with **Laplace smoothing** (just like before).  \n",
    "- At prediction time:\n",
    "  - We break the test sentence into bigrams.  \n",
    "  - We calculate scores for each class using those bigram probabilities.  \n",
    "  - The class with the highest score is chosen.\n",
    "\n",
    "###  Your Task\n",
    "1. Implement a **`BigramNaiveBayes`** class.  \n",
    "2. Train your model on the training data.  \n",
    "3. Generate predictions on the test set.  \n",
    "4. Evaluate your model using the **accuracy, precision, recall, F1, and confusion matrix** code you already have.\n",
    "\n",
    "### Hint:\n",
    "You can start by writing a helper function to turn a sentence into bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
